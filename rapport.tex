\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{hyperref}

\title{Virtualisation de performances}
\author{Aurèle Barrière \& Benjamin Fasquelle}

\begin{document}
\maketitle
\def\code#1{\texttt{#1}}

\paragraph{Remarque} Lorsque ce n'est pas précisé dans notre réponse, il ne faut pas comparer les résultats d'une question avec ceux d'une autre question. En effet, les mesures ont été faites lors de différentes réservations sur des machines différentes.

\section{Préambule}
Après avoir mis une adresse IP valide, il est bien possible de se connecter à la machine virtuelle avec \code{virsh console wheezy0} et installer \code{htop} (avec \code{apt-get} par exemple).

\section{Performances CPU}

\paragraph{Question 2.1.} Nous avons lancé le programme \texttt{hanoi} avec 15,17 et 18 comme arguments sur la machine virtuelle (créée avec \code{virsh}) et sur la Debian hôte. Nous sommes ainsi garantis que le programme tourne bien sur le même matériel physique.

Le temps d'exécution, en secondes est le suivant:

\begin{tabular}{| l | c | c | c |}\hline
Machine & $N=15$ & $N=17$ & $N=18$ \\\hline
Machine virtuelle & 10.169 & 40.975 & 181.521 \\\hline
Debian hôte & 0.132 & 0.408 & 0.840\\\hline
\end{tabular}

Il y a donc une différence de performance significative. Pour ces arguments, la machine virtuelle est entre 77 et 216 fois plus lente que sa machine hôte. Cela provient de la couche de virtualisation.

\paragraph{Question 2.2.} 
Cette fois-ci, nous comparons l'exécution sur deux machines virtuelles sur le même serveur à l'exécution sur une unique machine virtuelle. Les machines utilisées ont une configuration identique.

Nous lançons le programme \code{hanoi} avec \code{N=20}. Sur une seule machine virtuelle, le programme termine en 2 minutes 45 secondes.

Sur deux machines virtuelles, les programmes terminent en 2 minutes 50 et 2 minutes 51.


Le temps d'exécution est donc du même ordre qu'avec une seule machine virtuelle. En utilisant \code{htop}, nous avons pu observer l'allocation des ressources CPU aux deux machines virtuelles exécutant \texttt{hanoi}. Nous avons pu voir que le CPU associé à l'une des machines virtuelles est dynamique: il était différent à chaque exécution, et changeait même pendant l'exécution. De ce fait, les exécutions parallèles d'\texttt{hanoi} ne s'exécutaient pas sur le même CPU, ce qui explique que le temps d'exécution (en parallèle) n'a pas presque pas augmenté par rapport à une exécution simple.

Nous avons effectué d'autres mesures sur des serveurs différents, et la différence n'a jamais dépassé 4\%.


\paragraph{Question 2.3.}
On utilise la commande \code{virsh vcpupin wheezy0 0 0} pour mettre les deux machines virtuelles sur le même CPU.

\begin{tabular}{| l | c |}\hline
Utilisation des CPU & $N=20$ \\\hline
Sans forcer le CPU &  \\\hline
En forçant le CPU & 1.580 \\\hline
\end{tabular}


Cette fois-ci, les temps d'exécution sont bien supérieurs: 7 minutes 30 et 7 minutes 31 sur deux machines contre 3 minutes 10 sur une seule machine. Le temps d'exécution est donc plus de deux fois supérieur.

Avec \code{htop}, on constate que 4 c\oe urs sont utilisés presque au maximum. Cette fois-ci, les CPUs utilisés ne changent pas au cours de l'exécution. Cela explique le temps doublé.

Nous avons essayé sur d'autres serveurs, et le temps a toujours été quelques secondes au dessus du double d'une exécution simple.


\section{Performances réseau}
\paragraph{Question 3.1.}
On note les résultats de la commande \code{ping}. Dans tous les cas testés, il n'y a eu aucune perte de paquets. On ne notera donc que la dernière ligne de \code{ping}.
\\

Entre deux VMs hébergées sur le même serveur:

\code{rtt min/avg/max/mdev = 0.235/0.286/0.378/0.047 ms}
\\

Entre deux VMs hébergées sur des serveur différents:

\code{rtt min/avg/max/mdev = 0.354/0.444/0.819/0.145 ms}
\\

Entre deux serveurs différents:

\code{rtt min/avg/max/mdev = 0.088/0.103/0.190/0.034 ms}
\\

Entre un OS hôte et une VM sur le même serveur:

\code{rtt min/avg/max/mdev = 0.240/0.296/0.419/0.051 ms}
\\

On remarque que la communication entre serveurs est la plus rapide, tandis que celles avec les machines virtuelles sont les plus lentes. On peut en déduire que les machines virtuelles ralentissent les communications.

On peut également observer que les discussions entre les deux machines virtuelles sont plus rapides lorqu'elles sont hébergées sur le même serveur, on peut donc en déduire qu'il y a une communication interne au serveur, plus rapide que celle entre serveurs.

Cependant, ces mesures n'ont été faites qu'entre deux serveurs particuliers. Il faudrait faire plus de mesures sur des serveurs différents pour être plus convaincu par ces déductions. 

\paragraph{Question 3.2.}
On donne les résultats de la commande \code{iperf}.
\\

Pour des machines hébergées sur le même serveur:

\code{[  3]  0.0-10.0 sec  2.76 GBytes  2.37 Gbits/sec}
\\

Pour des machines hébergées sur des serveurs différents:

\code{[  3]  0.0-10.0 sec  1.10 GBytes   941 Mbits/sec}
\\

Il y a donc une différence significative de bande passante (avec un facteur d'environ 2.5). Sans surpise, des machines hébergées sur le même serveur ont une bande passante bien supérieure à deux machines hébergées sur des serveurs différents.

ON N A PAS SATURE LE RESEAU

En émettant du trafic avec toutes les machines virtuelles, on parvient facilement à saturer le réseau.

\section{Migrons}
\paragraph{Question 4.1.}


Avec la commande \code{virsh migrate}, nous avons migré une machine virtuelle d'un serveur à un autre. Cette opération est rapide (entre 2 et 5 secondes selon les serveurs utilisés).

On utilise la commande \code{virsh migrate wheezy0 qemu+ssh://nodename/system}.

\paragraph{Question 4.2.}

On peut migrer deux machines virtuelles en parrallèle d'un même serveur source vers un même serveur destination. Comme attendu, l'opération est plus longue que lorsqu'on migre une seule machine virtuelle (par exemple, 7.850 secondes pour le serveur qui ne mettait que 5.840 secondes à faire une migration simple).


On peut également migrer en parallèle deux machines virtuelles depuis des serveurs sources différents. Ici, l'opération est environ aussi rapide que pour une migration simple (6.061 secondes pour le serveur qui en mettait 5.840 à faire une migration simple).



\paragraph{Question 4.3.}

On peut migrer en parallèle une machine virtuelle d'un serveur A vers un serveur B, et une autre du serveur B vers le serveur A. Comme attendu, le temps de migration est environ égal au temps maximal que met un des serveurs à migrer sa machine. 

\paragraph{Question 4.4.}

On crée un fichier d'une taille fixe avec la commande \code{dd if=/dev/zero of=file bs=1G count=1}. Nous avons pu constater une forte augmentation du temps de migration (de 2.262 secondes à 6.795 secondes). 


\paragraph{Question 4.5.}

\def\vcpu{\textit{v}CPU}

Le temps de migration semble dépendre de ces deux paramètres.

Commençons par changer le nombre de \vcpu s. Sur un serveur qui fait une migration en 2.262 secondes pour 1 \vcpu, on obtient 2.31 secondes pour 3 \vcpu s (soit une augmentation de 0.15 secondes) et 3.37 secondes pour 15 \vcpu s (soit une augmentation de 1.1 secondes).

Ensuite, on fait varier la taille de la mémoire.
Pour une taile de $2^{19}$ bits, on a le même temps de 2.262 secondes.
Pour $2^{22}$ bits, on a 4.938 secondes.
Pour $2^{11}$ bits, on a 1.188 secondes.

Le temps de migration semble donc augmenter avec la taille de la mémoire et le nombre de \vcpu s.


\paragraph{Question 4.6.}

Nous proposons d'effectuer un calcul sur une machine virtuelle en notant son temps d'exécution, puis de refaire le même calcul avec une migration pendant son exécution. La différence des temps d'exécution devrait nous donner le temps d'interruption.

On utilise l'option \code{--live} de \code{virsh migrate}.

Une premier test sur deux serveurs nous donnent une temps de migration de 1.33 secondes (5.74 et 7.07 secondes).

Un deuxième test sur des serveurs différents avec un programme plus long nous donne 1.31 seondes (178.28 et 179.29) secondes.

Comme attendu, ce temps de migration semble donc constant quel que soit le temps d'exécution du programme.


\end{document}
